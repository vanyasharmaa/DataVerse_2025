{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8a87c1-7841-4250-a792-d8b2a045224b",
   "metadata": {},
   "source": [
    "# Workshop #2 Data Analysis\n",
    "## USS X WIDS DataVerse\n",
    "### By Vanya Sharma ( President - USS ) & Erhan Javed (VP Marketing - USS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877cc176-6150-4768-80ed-c8eb36ce617a",
   "metadata": {},
   "source": [
    "# Introduction to the Dataset\n",
    "\n",
    "*   We use the [Default of Credit Card Clients Dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset)\n",
    "*   The dataset contains 30,000 observations and 24 features.\n",
    "*   The goal is to predict whether a client will default(fail to pay) on their next monthâ€™s credit card payment.\n",
    "*   The target variable is 'default.payment.next.month' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95de135-1f99-400c-bb8f-ece341c88e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection as sk\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9fc433f-a974-43aa-81c0-34859aa789cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV (student answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c90a1-ae4b-42cc-bf1f-977c03fcae5b",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Step 1 â€” Splitting the Dataset\n",
    "\n",
    "**Why splitting is needed ?**\n",
    "\n",
    "We split Data into Training Data and Testing Data.\n",
    "\n",
    "*   Ensures the model can generalize to unseen data and not just memorize the training data.\n",
    "*   Helps detect overfitting (eg : perfect prediction results !) by comparing training vs. test performance.\n",
    "\n",
    "\n",
    "**Common methods for splitting**\n",
    "\n",
    "\n",
    "\n",
    "*   Trainâ€“Test Split: Simple and fast; works well for large datasets.\n",
    "*   K-Fold Cross-Validation: More reliable performance estimates; ideal for small/medium datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6ded01-258e-46de-ad46-b3de5fe87a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets Split the data 80-20 using `train_test_split` provided in `scikit-learn`\n",
    "# (student answer here - uncomment and run the code)\n",
    "# train_df, test_df = sk.train_test_split(df, test_size=0.2, random_state=123)\n",
    "# train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eada1876-cf21-4da3-8ada-cccb69597762",
   "metadata": {},
   "source": [
    " ## ðŸ“Œ Step 2 â€” Understanding the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bcbee68-7cf6-4bc5-b509-d716d65ab005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DataFrame , provide a concise summary/info\n",
    "# (student answer here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "542ee127-7c86-42ec-9fa4-586ae969b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the proportion of each unique value in the target variable `default.payment.next.month`\n",
    "# (student answer here - uncomment and run code)\n",
    "# print(\n",
    "#     \"Fraction that default:\\n\",\n",
    "#     train_df[\"default.payment.next.month\"].value_counts(normalize=True),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8b1a92e-e616-417e-9999-a95865d8f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate minimum and maximum of data\n",
    "# (student answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f570c6ed-0433-4c78-a5ad-10d04ddc70c3",
   "metadata": {},
   "source": [
    "We will now separate:\n",
    "\n",
    "- `X_train` â€” features used for training\n",
    "- `y_train` â€” target used for training\n",
    "- `X_test` â€” features used for testing\n",
    "- `y_test` â€” target used for checking accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b00e1-a3ee-4f90-b67c-457ca4ee6a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this code\n",
    "# X_train, y_train = (\n",
    "#     train_df.drop(columns=[\"default.payment.next.month\"]),\n",
    "#     train_df[\"default.payment.next.month\"],\n",
    "# )\n",
    "# X_test, y_test = (\n",
    "#     test_df.drop(columns=[\"default.payment.next.month\"]),\n",
    "#     test_df[\"default.payment.next.month\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add56f90-d13c-46c3-9e47-3e035ef414f2",
   "metadata": {},
   "source": [
    "#### Correlation Heatmap\n",
    "\n",
    "The correlation heatmap visually represents the pairwise correlation coefficients between all features in the dataset, including the target variable `default.payment.next.month`. Each cell in the heatmap shows the correlation between two variables, with colors indicating the strength and direction of the relationship. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470dc682-4a99-426b-899f-c99b11eb1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and fill the blanks\n",
    "\n",
    "# cor = pd.concat((..., ...), axis=1).iloc[:, :30].corr()\n",
    "# plt.figure(figsize=(30, 30))\n",
    "# sns.set(font_scale=1)\n",
    "# sns.heatmap(cor, annot=True, cmap=plt.cm.Reds);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca9ae6-cc69-411a-9f5e-e0b83ee2a9fd",
   "metadata": {},
   "source": [
    "#### Feature Histograms\n",
    "\n",
    "Histograms display the distribution of individual numerical features. They divide the data into bins and count the number of data points that fall into each bin, showing the frequency of values within different ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300014b2-d5ca-41c3-9b15-9a5ac546c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the code and run the cell\n",
    "\n",
    "# train_df.hist(figsize=(20, 20), bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d294b-f040-4b8f-8b94-433e12357ec9",
   "metadata": {},
   "source": [
    "We see quite a few outliers for features such as `EDUCATION`, `MARRIAGE`, and `PAY_\\d*` features.\n",
    "\n",
    "Some initial observations:\n",
    "    \n",
    "- We have very few features.\n",
    "- We have class imbalance and we need to deal with it. We have chosen macro average f1 as our target metric so that both classes get equal weight.\n",
    "- The feature ranges are very different, so we'll need to standardize.\n",
    "- We have a number of collinear features.\n",
    "- We have quite a few outliers.\n",
    "- The data is messy / doesn't always correspond to the data description.\n",
    "  - What are education levels 5 and 6?\n",
    "  - What does it mean for PAY_* to be -2? Or 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6bbfc-0877-4634-8df7-b6314cf23b3f",
   "metadata": {},
   "source": [
    " ## ðŸ“Œ Step 3 â€” Choosing the right Data Model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4e0d7-98e9-4277-9112-9d510deab3d0",
   "metadata": {},
   "source": [
    "#### What is a Logistic Regression?\n",
    "Logistic Regression is a statistical model used for binary classification tasks (predicting one of two outcomes). Unlike Linear Regression, which is used for predicting continuous values, Logistic Regression predicts the probability that an instance belongs to a certain class. It achieves this by using a sigmoid function (also known as a logistic function) to map any real-valued number into a value between 0 and 1, which can then be interpreted as a probability.\n",
    "\n",
    "Key Differences from Linear Regression for Classification:\n",
    "\n",
    "- Output: Linear Regression outputs a continuous value, while Logistic Regression outputs a probability (between 0 and 1).\n",
    "- Function: Linear Regression uses a linear function, whereas Logistic Regression uses a sigmoid (logistic) function to transform the linear output.\n",
    "- Purpose: Linear Regression is for regression problems, while Logistic Regression is specifically designed for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a645b58b-634c-474e-a37b-30a8b025621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and fill in the blanks\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# log_model = ...(max_iter=1000)\n",
    "# log_model.fit(..., ...)\n",
    "\n",
    "# log_model.score(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d69c1e-786c-4bfb-8052-ebd8a95a0f36",
   "metadata": {},
   "source": [
    "\n",
    "Interpreting the Model Score\n",
    "\n",
    "For sklearn classification models like LogisticRegression, the .score() method, when called with X_test and y_test, typically returns the mean accuracy on the given test data and labels. This means it calculates the proportion of correctly predicted instances out of the total instances in the test set.\n",
    "\n",
    "In this case, the score of ... indicates that the Logistic Regression model correctly predicted the 'default.payment.next.month' for approximately ...% of the clients in the test_df dataset. This is a significantly better performance compared to the Linear Regression model, which yielded a much lower score, demonstrating Logistic Regression's suitability for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827385d-862b-43d9-ac6b-698171c66abc",
   "metadata": {},
   "source": [
    "#### Some other models to consider (we will not go into their detail , but feel free to learn and use them during the hackathon ! :)\n",
    "\n",
    "- Decision Trees / Random Forest\n",
    "- Gradient Boosting Models\n",
    "- Support Vector Machine (SVM)\n",
    "- Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5787a7b-adda-4aa2-9711-d4b9d154de2a",
   "metadata": {},
   "source": [
    " ## ðŸ“Œ Step 4 â€” Challenge Yourself \n",
    "\n",
    "Learn and try incorporating one or more of the 3 evaualation metrics in your project \n",
    "\n",
    "- ROC-AUC\n",
    "\n",
    "- Precision, Recall, F1-score\n",
    "\n",
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33455fcc-7860-43e8-9f46-7c94ec29cf71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
